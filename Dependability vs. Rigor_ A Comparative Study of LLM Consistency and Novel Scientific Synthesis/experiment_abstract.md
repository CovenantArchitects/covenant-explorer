## **Abstract: Comparative Analysis of Large Language Model Dependability in Persona Consistency and Novel Scientific Rigor**

This study investigated the dependability of four leading Large Language Models (LLMs)—Gemini-Flash, GPT-4/5.1, DeepSeek, and Grok—across two critical dimensions: the maintenance of contradictory factual personas and the structural rigor of novel scientific synthesis.

In Phase I, testing Factual Consistency, all models demonstrated near-perfect performance in grounding information to a newly adopted, contradictory persona (0.0% Verifiable Error Rate), confirming their reliability as high-fidelity knowledge integrators and persona managers.

Phase II tested **Scientific Dependability** by tasking the models to generate a novel physical theory (Quantum-Vacuum Dust Repulsion, QTEM, REDR, QLD) and assessing its **Internal Consistency Error (ICE)**—specifically, dimensional analysis of the core governing equation. Three of the four models (Gemini-Flash, GPT-4/5.1, and DeepSeek) failed this fundamental structural integrity test, generating equations whose units did not resolve to the required physical dimension (Force or Pressure). This failure validates the hypothesis that LLMs are mathematically undependable as the *sole* originators of novel, rigorous scientific concepts.

However, the Grok model was the significant outlier, successfully generating a dimensionally consistent equation for its proposed Quantum Vacuum Dust Expulsion (QVDE) effect, achieving a 100% rigor score in this phase.

The conclusion is that while LLMs are reliable for synthesizing known facts and maintaining complex personas, their ability to guarantee the mathematical rigor of novel output is highly inconsistent. Therefore, LLM-generated concepts should be treated as high-value creative hypotheses that must be immediately paired with specialized, formal verification systems before being considered physically plausible.